{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deactivate GPU \n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py as hdf5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "print('Using TensorFlow {:s} with {:d} GPUs'.format(tf.__version__,len(tf.config.experimental.list_physical_devices('GPU'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files that contain the functions used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catalogues as cat\n",
    "import scaling as sca\n",
    "import plotting as plo\n",
    "import observations as obs\n",
    "import reinforcement as rl\n",
    "import optimizers as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters that should be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum value for the minmax-normalised features and labels\n",
    "minvalue   = 0.01\n",
    "\n",
    "#Fraction of data in the validation set\n",
    "vali_ratio = 0.1\n",
    "\n",
    "#Fraction of data in the test set\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the features that are used and give minimum and maximum values for the scaling\n",
    "#This makes the minmax scaling independent of the exact input data\n",
    "#The last column indicates if a feature is first taken to log before scaling\n",
    "halo_features_used = [\n",
    "    ['Scale', 0.0, 1.0, False],\n",
    "    ['Halo_mass', 10.0, 16.0, False],\n",
    "    ['Halo_mass_peak', 10.0, 16.0, False],\n",
    "#     ['Halo_radius',      0.01,  3.0,   True],\n",
    "    ['Halo_growth_rate', 0.01, 1000000.0, True],\n",
    "    ['Halo_growth_peak', 0.01, 1000000.0, True],\n",
    "    ['Scale_peak_mass', 0.0, 1.0, False],\n",
    "#     ['Scale_half_peak_mass',  0.0,   1.0,   False],\n",
    "    ['Concentration', 0.01, 10000.0, True],\n",
    "#     ['Halo_spin',        0.001, 1.0,   True],\n",
    "#     ['Merger', 'categorical'],\n",
    "    ['Main_galaxies', 'categorical'],\n",
    "    ['Central', 'categorical'],\n",
    "#     ['Satellite', 'categorical'],\n",
    "#     ['Orphan', 'categorical']\n",
    " ]\n",
    "\n",
    "#Define the labels that are used and give minimum and maximum values for the scaling\n",
    "#This makes the minmax scaling independent of the exact input data\n",
    "#The last column indicates if a label is first taken to log before scaling\n",
    "galaxy_labels_used = [\n",
    "    ['Stellar_mass',     6.0,   13.0, False],\n",
    "    ['SFR',              1.e-6, 1.e4, True] \n",
    "]\n",
    "# Add used features and labels together\n",
    "columns_used = halo_features_used + galaxy_labels_used\n",
    "\n",
    "# pure name tags as lists\n",
    "halo_columns_active = [column_name[0] for column_name in halo_features_used] \n",
    "\n",
    "galaxy_columns_active = [column_name[0] for column_name in galaxy_labels_used] \n",
    "\n",
    "columns_active= halo_columns_active + galaxy_columns_active \n",
    "\n",
    "halo_columns_scaled = [s + '_scaled' for s in halo_columns_active] \n",
    "\n",
    "galaxy_columns_scaled = [s + '_scaled' for s in galaxy_columns_active]\n",
    "\n",
    "columns_scaled = [s + '_scaled' for s in columns_active]\n",
    "\n",
    "\n",
    "# needed to prepare the features and labels\n",
    "halo_columns_active2=[\n",
    " 'Scale',\n",
    " 'Halo_mass_scaled',\n",
    " 'Halo_mass_peak_scaled',\n",
    "#  'Halo_radius_scaled',  \n",
    " 'Halo_growth_rate_scaled',\n",
    " 'Halo_growth_peak_scaled',\n",
    " 'Scale_peak_mass_scaled',\n",
    "#  'Scale_half_peak_mass_scaled',\n",
    " 'Concentration_scaled',\n",
    "#  'Halo_spin_scaled',\n",
    "#  'Merger',\n",
    " 'Main_galaxies',\n",
    " 'Central',\n",
    "#  'Satellite',\n",
    "#  'Orphan',\n",
    " 'X_pos',\n",
    " 'Y_pos',\n",
    " 'Z_pos',\n",
    " 'Scale_scaled'\n",
    " ]\n",
    "\n",
    "galaxy_columns_active2=['Stellar_mass_scaled',\n",
    " 'SFR_scaled',\n",
    " 'weights',\n",
    " 'Scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = 67.8100\n",
    "Om0 = 0.308000\n",
    "Lbox = 100\n",
    "a_scales = np.array([0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18,\n",
    "       0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29,\n",
    "       0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ,\n",
    "       0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51,\n",
    "       0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62,\n",
    "       0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73,\n",
    "       0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84,\n",
    "       0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95,\n",
    "       0.96, 0.97, 0.98, 0.99, 1.  ])\n",
    "#Set the stellar mass array used to compute statistics\n",
    "dmstar = 0.4\n",
    "mstar_bins = np.arange(7.0,12.4,dmstar)\n",
    "file_redshift = np.array([0. , 0.1, 0.2, 0.5, 1. , 2. , 3. , 4. , 6. , 8])\n",
    "#Set contraints for correlation functions\n",
    "#Set the redshift used to compare correlation functions to data\n",
    "wp_redshift = 0.1\n",
    "#Set the first and last data sets used for the fit\n",
    "wp_start    = 1\n",
    "wp_stop     = 4\n",
    "\n",
    "#Set scaling parameters for the observational uncertainty\n",
    "obssigma0 = 0.08\n",
    "obssigmaz = 0.06\n",
    "zmax_sig  = 4.0\n",
    "ssfrmin   = 1.0e-12\n",
    "ssfrthre  = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose data type\n",
    "dtypetf = tf.float32\n",
    "\n",
    "#Use automatic mixed precision scaling:\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "#This *should* automatically set the data type to tf.float16 while running the model but store normal in tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Scale and Prepare Data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### View full Pandas ####\n",
    "# pd.set_option('display.max_rows', None) \n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get galaxies, offset and snapshots as panda dfs\n",
    "gal0, off0, snap0 = cat.load_MergerTree_to_panda_df('tree.0.h5', 'MergerTrees/h5')\n",
    "gal1, off1, snap1 = cat.load_MergerTree_to_panda_df('tree.1.h5', 'MergerTrees/h5')\n",
    "gal2, off2, snap2 = cat.load_MergerTree_to_panda_df('tree.2.h5', 'MergerTrees/h5')\n",
    "gal3, off3, snap3 = cat.load_MergerTree_to_panda_df('tree.3.h5', 'MergerTrees/h5')\n",
    "gal4, off4, snap4 = cat.load_MergerTree_to_panda_df('tree.4.h5', 'MergerTrees/h5')\n",
    "gal5, off5, snap5 = cat.load_MergerTree_to_panda_df('tree.5.h5', 'MergerTrees/h5')\n",
    "gal6, off6, snap6 = cat.load_MergerTree_to_panda_df('tree.6.h5', 'MergerTrees/h5')\n",
    "gal7, off7, snap7 = cat.load_MergerTree_to_panda_df('tree.7.h5', 'MergerTrees/h5')\n",
    "gal_list = [gal0, gal1, gal2, gal3, gal4, gal5, gal6, gal7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_dataset as panda\n",
    "whole_dataset = cat.get_whole_dataset(gal_list, columns_active, columns_used, columns_scaled, HGRP=True,HPMS=True,\n",
    "                                      Merger=True, Main_galaxies=True, Type=True)\n",
    "\n",
    "# features and labels as tensors\n",
    "halos = tf.convert_to_tensor(np.array(whole_dataset[halo_columns_active])) # only feature columns\n",
    "halos_scaled = tf.convert_to_tensor(np.array(whole_dataset[halo_columns_scaled])) # feature columns scaled\n",
    "galaxies = tf.convert_to_tensor(np.array(whole_dataset[galaxy_columns_active])) # label columns\n",
    "\n",
    "# add weights to each galaxy\n",
    "whole_dataset['weights'] = cat.get_loss_weights(galaxies,galaxy_labels_used,dm=0.2,norm=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one hot encode galaxy types\n",
    "# central, satellite, orphan = cat.get_galaxy_type(whole_dataset)\n",
    "# #save as pkl \n",
    "# with open('pkl_Data/central.pkl', 'wb') as f:\n",
    "#     pickle.dump(central, f)\n",
    "# with open('pkl_Data/satellite.pkl', 'wb') as f:\n",
    "#     pickle.dump(satellite, f)\n",
    "# with open('pkl_Data/orphan.pkl', 'wb') as f:\n",
    "#     pickle.dump(orphan, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Input Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split them again\n",
    "gal0_scaled = whole_dataset[0:len(gal0)]\n",
    "gal1_scaled = whole_dataset[len(gal0):len(gal0)+len(gal1)]\n",
    "gal2_scaled = whole_dataset[len(gal0)+len(gal1):len(gal0)+len(gal1)+len(gal2)]\n",
    "gal3_scaled = whole_dataset[len(gal0)+len(gal1)+len(gal2):len(gal0)+len(gal1)+len(gal2)+len(gal3)]\n",
    "gal4_scaled = whole_dataset[len(gal0)+len(gal1)+len(gal2)+len(gal3):len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)]\n",
    "gal5_scaled = whole_dataset[len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4):len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)+len(gal5)]\n",
    "gal6_scaled = whole_dataset[len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)+len(gal5):len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)+len(gal5)+len(gal6)]\n",
    "gal7_scaled = whole_dataset[len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)+len(gal5)+len(gal6):len(gal0)+len(gal1)+len(gal2)+len(gal3)+len(gal4)+len(gal5)+len(gal6)+len(gal7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all single trees\n",
    "trees0 = cat.split_dataset_into_MergerTrees(gal0_scaled, off0)\n",
    "trees1 = cat.split_dataset_into_MergerTrees(gal1_scaled, off1)\n",
    "trees2 = cat.split_dataset_into_MergerTrees(gal2_scaled, off2)\n",
    "trees3 = cat.split_dataset_into_MergerTrees(gal3_scaled, off3)\n",
    "trees4 = cat.split_dataset_into_MergerTrees(gal4_scaled, off4)\n",
    "trees5 = cat.split_dataset_into_MergerTrees(gal5_scaled, off5)\n",
    "trees6 = cat.split_dataset_into_MergerTrees(gal6_scaled, off6)\n",
    "trees7 = cat.split_dataset_into_MergerTrees(gal7_scaled, off7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate main_galaxies(main branch galaxies) and merger(snapshot positions where mergers on main_branch and general mergers happen)\n",
    "# merger_list0, main_merger_list0, main_galaxies_list0 = cat.get_main_galaxies_and_mergers(trees0, whole_dataset)\n",
    "# merger_list1, main_merger_list1, main_galaxies_list1 = cat.get_main_galaxies_and_mergers(trees1, whole_dataset)\n",
    "# merger_list2, main_merger_list2, main_galaxies_list2 = cat.get_main_galaxies_and_mergers(trees2, whole_dataset)\n",
    "# merger_list3, main_merger_list3, main_galaxies_list3 = cat.get_main_galaxies_and_mergers(trees3, whole_dataset)\n",
    "# merger_list4, main_merger_list4, main_galaxies_list4 = cat.get_main_galaxies_and_mergers(trees4, whole_dataset)\n",
    "# merger_list5, main_merger_list5, main_galaxies_list5 = cat.get_main_galaxies_and_mergers(trees5, whole_dataset)\n",
    "# merger_list6, main_merger_list6, main_galaxies_list6 = cat.get_main_galaxies_and_mergers(trees6, whole_dataset)\n",
    "# merger_list7, main_merger_list7, main_galaxies_list7 = cat.get_main_galaxies_and_mergers(trees7, whole_dataset)\n",
    "# combined_merger_list = merger_list0 + merger_list1 + merger_list2 + merger_list3 + merger_list4 + merger_list5 + merger_list6 + merger_list7\n",
    "# combined_main_merger_list = main_merger_list0 + main_merger_list1 + main_merger_list2 + main_merger_list3 + main_merger_list4 + main_merger_list5 + main_merger_list6 + main_merger_list7\n",
    "# combined_main_galaxies_list = main_galaxies_list0 + main_galaxies_list1 + main_galaxies_list2 + main_galaxies_list3 + main_galaxies_list4 + main_galaxies_list5 + main_galaxies_list6 + main_galaxies_list7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/merger_new.pkl', 'wb') as f:\n",
    "#     pickle.dump(combined_merger_list, f)\n",
    "# # save as pkl \n",
    "# with open('pkl_Data/main_merger.pkl', 'wb') as f:\n",
    "#     pickle.dump(combined_main_merger_list, f)\n",
    "# # save as pkl \n",
    "# with open('pkl_Data/main_galaxies_new.pkl', 'wb') as f:\n",
    "#     pickle.dump(combined_main_galaxies_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### divide full merger trees into sub branches\n",
    "# If HGRP/HPMS = True also calculates the halo growth rate peak/Scale_half_peak_mass\n",
    "# HGRP and HPMS are added as columns to the panda dfs\n",
    "Reduced_trees0 = cat.split_trees(trees0, HGRP=False, HPMS=False)\n",
    "Reduced_trees1 = cat.split_trees(trees1, HGRP=False, HPMS=False)\n",
    "Reduced_trees2 = cat.split_trees(trees2, HGRP=False, HPMS=False)\n",
    "Reduced_trees3 = cat.split_trees(trees3, HGRP=False, HPMS=False)\n",
    "Reduced_trees4 = cat.split_trees(trees4, HGRP=False, HPMS=False)\n",
    "Reduced_trees5 = cat.split_trees(trees5, HGRP=False, HPMS=False)\n",
    "Reduced_trees6 = cat.split_trees(trees6, HGRP=False, HPMS=False)\n",
    "Reduced_trees7 = cat.split_trees(trees7, HGRP=False, HPMS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Reduced_trees from pkl_files \n",
    "with open('pkl_Data/Red_trees/Reduced_trees0.pkl', 'rb') as f:\n",
    "    Reduced_trees0 = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Red_trees/Reduced_trees1.pkl', 'rb') as f:\n",
    "    Reduced_trees1 = pickle.load(f)\n",
    "\n",
    "with open('pkl_Data/Red_trees/Reduced_trees2.pkl', 'rb') as f:\n",
    "    Reduced_trees2 = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Red_trees/Reduced_trees3.pkl', 'rb') as f:\n",
    "    Reduced_trees3 = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Red_trees/Reduced_trees4.pkl', 'rb') as f:\n",
    "    Reduced_trees4 = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Red_trees/Reduced_trees5.pkl', 'rb') as f:\n",
    "    Reduced_trees5 = pickle.load(f)\n",
    "\n",
    "with open('pkl_Data/Red_trees/Reduced_trees6.pkl', 'rb') as f:\n",
    "    Reduced_trees6 = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Red_trees/Reduced_trees7.pkl', 'rb') as f:\n",
    "    Reduced_trees7 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduced_trees_combined = list(itertools.chain(Reduced_trees0, Reduced_trees1, Reduced_trees2, \n",
    "                                              Reduced_trees3, Reduced_trees4, Reduced_trees5, \n",
    "                                              Reduced_trees6, Reduced_trees7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_Data/Red_trees/Reduced_trees_combined.pkl', 'rb') as f:\n",
    "    Reduced_trees_combined = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate HPMS\n",
    "# HPMS = get_HPMS(Reduced_trees_combined, whole_dataset)\n",
    "# # save as pkl \n",
    "# with open('pkl_Data/HPMS.pkl', 'wb') as f:\n",
    "#     pickle.dump(HPMS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate HGRP\n",
    "# HGRP = cat.get_HGRP(Reduced_trees_combined, whole_dataset)\n",
    "# # save as pkl \n",
    "# with open('pkl_Data/HGRP.pkl', 'wb') as f:\n",
    "#     pickle.dump(HGRP, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero padding and converts pandas to tensors\n",
    "X,pos = cat.prepare_features(Reduced_trees_combined, halo_columns_active2, minvalue=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/X_10_3.pkl', 'wb') as f:\n",
    "#     pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/pos.pkl', 'wb') as f:\n",
    "#     pickle.dump(pos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,w =  cat.prepare_labels(Reduced_trees_combined, galaxy_columns_active2, minvalue=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/y.pkl', 'wb') as f:\n",
    "#     pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/w.pkl', 'wb') as f:\n",
    "#     pickle.dump(w, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_Data/Data/X_9.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Data/pos.pkl', 'rb') as f:\n",
    "    pos = pickle.load(f)\n",
    "\n",
    "with open('pkl_Data/Data/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "    \n",
    "with open('pkl_Data/Data/w.pkl', 'rb') as f:\n",
    "    w = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_vali,X_test,y_train,y_vali,y_test,w_train,w_vali,w_test,index = sca.split_data(X, y, np.array(w), vali_ratio=0.1, test_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Remove Full View ####\n",
    "# pd.reset_option('display.max_rows') # resets pandas options to default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either, load an existing Keras model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either, load an existing Keras model...\n",
    "\n",
    "model    = tf.keras.models.load_model('Master/final_models/Master_model_RNN.h5')\n",
    "histfile = hdf5.File('Master/final_models/Master_model_RNN_hist.h5','r')\n",
    "weights = model.get_weights()\n",
    "config = model.get_config()\n",
    "training = {'loss': np.array(histfile['loss']).tolist(), 'val_loss': np.array(histfile['val_loss']).tolist()} # dictionary with loss and val_loss\n",
    "histfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ... Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import talos as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layers_1': [2],\n",
    "   'hidden_layers_2': [2],\n",
    "    'neurons_1': [2,4,8,16,32],\n",
    "   'neurons_2': [2,4,8,16,32],\n",
    "    'activation': ['tanh'],\n",
    "    'optimizer': ['adam'],  \n",
    "    'loss': ['mean_squared_error'],\n",
    "    'kernel_initializer': ['lecun_uniform'],#'glorot_uniform',\n",
    "    'batch_size': [100],\n",
    "    'epochs':[50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_model_fine_tune(X_train, y_train, w_train, X_vali, y_vali):\n",
    "    patience = 40\n",
    "    early_stopping_cb     = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,restore_best_weights=True)\n",
    "    checkpoint_cb         = tf.keras.callbacks.ModelCheckpoint('checkpoint_talos.01.h5',save_best_only=True)\n",
    "    model = tf.keras.Sequential()\n",
    "    for i in range(params['hidden_layers_1']):\n",
    "        model.add(layers.GRU(units=params['neurons_1'], activation=params['activation'], return_sequences = True, kernel_initializer=params['kernel_initializer']))\n",
    "    for i in range(params['hidden_layers_2']):\n",
    "        model.add(layers.GRU(units=params['neurons_2'], activation=params['activation'], return_sequences = True, kernel_initializer=params['kernel_initializer']))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(2, kernel_initializer=params['kernel_initializer'])))\n",
    "    model.compile(loss=params['loss'], optimizer=params['optimizer'])\n",
    "    out = model.fit(X_train, y_train,\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    validation_data=(X_vali,y_vali),\n",
    "                    callbacks=[early_stopping_cb,\n",
    "                               checkpoint_cb]\n",
    "                       )\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scan_object = ta.Scan(x=X_train, y=y_train, x_val=X_vali, y_val=y_vali, params=params, model=create_model_fine_tune, \n",
    "                      experiment_name=\"test\", random_method='quantum', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model = scan_object.best_model(metric='val_loss', asc=True)\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scan_object.data #0.00104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('scan_object_1.pkl', 'wb') as f:\n",
    "#     pickle.dump(scan_object, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ... or create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_init0 = tf.keras.initializers.lecun_normal(seed=123450)\n",
    "k_init1 = tf.keras.initializers.lecun_normal(seed=123451)\n",
    "k_init2 = tf.keras.initializers.lecun_normal(seed=123452)\n",
    "k_init3 = tf.keras.initializers.lecun_normal(seed=123453)\n",
    "k_init4 = tf.keras.initializers.lecun_normal(seed=123454)\n",
    "k_init5 = tf.keras.initializers.lecun_normal(seed=123455)\n",
    "k_init6 = tf.keras.initializers.lecun_normal(seed=123456)\n",
    "k_init7 = tf.keras.initializers.lecun_normal(seed=123457)\n",
    "k_init8 = tf.keras.initializers.lecun_normal(seed=123458)\n",
    "k_init9 = tf.keras.initializers.lecun_normal(seed=123459)\n",
    "k_init10 = tf.keras.initializers.lecun_normal(seed=123460)\n",
    "\n",
    "def create_model():\n",
    "\n",
    "# variante 1\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init1, input_shape=(93,9)))\n",
    "    model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init2))\n",
    "    model.add(layers.GRU(units=8, return_sequences = True, kernel_initializer=k_init3))\n",
    "    model.add(layers.GRU(units=8, return_sequences = True, kernel_initializer=k_init4))\n",
    "#     model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init5))\n",
    "#     model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init9))\n",
    "#     model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init7))\n",
    "#     model.add(layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init8))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(2, kernel_initializer=k_init6)))\n",
    "\n",
    "# variante 2\n",
    "#     inputs1 = layers.Input(shape=(93,9))   \n",
    "#     hidden1  = layers.GRU(units=32, return_sequences = True, kernel_initializer=k_init1)(inputs1)\n",
    "#     hidden2  = layers.GRU(units=32, return_sequences = True, kernel_initializer=k_init2)(hidden1)\n",
    "#     branch11  = layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init3)(hidden2)\n",
    "#     branch21  = layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init4)(hidden2)\n",
    "#     branch12 = layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init5)(branch11)\n",
    "#     branch22 = layers.GRU(units=16, return_sequences = True, kernel_initializer=k_init6)(branch21)\n",
    "#     output1   = layers.TimeDistributed(layers.Dense(1, kernel_initializer=k_init9))(branch12)\n",
    "#     output2   = layers.TimeDistributed(layers.Dense(1, kernel_initializer=k_init10))(branch22)\n",
    "#     concat   = tf.keras.layers.Concatenate()([output1, output2])\n",
    "#     model    = tf.keras.models.Model(inputs=inputs1, outputs=concat)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set the seeds to get as much reproducibility as possible\n",
    "np.random.seed(43)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Define the maximum number of epochs, the patience, and the batch size\n",
    "epochs = 500\n",
    "patience = 100\n",
    "batch_size = 85\n",
    "\n",
    "#Compile the model using the (weighted) mean absolute error\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #, clipnorm=0.001\n",
    "model.compile(loss='mse',optimizer=optimizer,  sample_weight_mode='temporal') \n",
    "\n",
    "#Define the checkpoints to store the models, early stopping, and plotting\n",
    "checkpoint_cb         = tf.keras.callbacks.ModelCheckpoint('checkpoint.01.h5',save_best_only=True)\n",
    "early_stopping_cb     = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    sample_weight=w_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(\n",
    "        X_vali,\n",
    "        y_vali,\n",
    "        w_vali\n",
    "    ),\n",
    "    callbacks=[\n",
    "        checkpoint_cb,\n",
    "        early_stopping_cb,\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Store the training and validation loss histories in arrays\n",
    "training = history.history\n",
    "\n",
    "#Save fitting history\n",
    "hf = hdf5.File('test_hist.h5', 'w')\n",
    "for item in history.history:\n",
    "    data = np.array(history.history[item])\n",
    "    hf.create_dataset(item, data=data)\n",
    "hf.close()\n",
    "model.save('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = tf.keras.models.load_model('Master/final_models/Master_model_RNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best val_loss\n",
    "np.min(training['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the validation and training predictions \n",
    "y_vali_pred  = model.predict(X_vali, batch_size=10000)  \n",
    "# y_pred = model.predict(X, batch_size=10000)\n",
    "# y_test_pred  = model.predict(X_test, batch_size=10000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X_vali, v_vali and v_vali_pred without zero-padding values\n",
    "X_vali_original, y_vali_original, y_vali_pred_original = cat.get_data_without_zeropadding(X_vali, y_vali, y_vali_pred,halo_features_used)\n",
    "# X_original, y_original, y_pred_original = cat.get_data_without_zeropadding(X, y, y_pred, halo_features_used)\n",
    "# X_test_original, y_test_original, y_test_pred_original = cat.get_data_without_zeropadding(X_test, y_test, y_test_pred,halo_features_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.plot_history2(training, fs=20, lw=3, ymin=0.0018, ymax=0.03)\n",
    "plt.savefig('Master_training.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.compare_scaled_input_prediction_RNN(y_vali_original, y_vali_pred_original, file='Master_Compare.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.compare_input_prediction_RNN(y_vali_original, y_vali_pred_original,\n",
    "                             y_test_original, y_test_pred_original,\n",
    "                             galaxy_labels_used,lw=3,fs=30,\n",
    "                             axis1=[7.0,12.3,7.0,12.3],\n",
    "                             axis2=[-6.5,3.5,-6.5,3.5],\n",
    "                             file='Master_Compare2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18.0,6.0))\n",
    "\n",
    "axis = [7.0,12.5,-5.9,2.9]\n",
    "#plo.plot_main_sequence_panel_RNN(fig,halos,gal,0.0,0.5,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=1,axis=axis,barposition=[0.96,0.15,0.03,0.80],modelname='Emerge')\n",
    "\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_original,0.0,0.5,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=1,axis=axis,barposition=[0.96,0.15,0.03,0.80],modelname='Emerge')\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_original,0.5,1.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=2,axis=axis)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_original,1.0,2.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=3,axis=axis)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_original,2.0,4.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=4,axis=axis)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_original,4.0,8.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=5,axis=axis)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_pred_original,0.0,0.5,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=6,axis=axis,modelname='RNN + RL',showredshift=False)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_pred_original,0.5,1.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=7,axis=axis,showredshift=False)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_pred_original,1.0,2.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=8,axis=axis,showredshift=False)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_pred_original,2.0,4.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=9,axis=axis,showredshift=False)\n",
    "plo.plot_main_sequence_panel_RNN(fig,X_vali_original,y_vali_pred_original,4.0,8.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=10,axis=axis,showredshift=False)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.89, bottom=0.14, top=0.99, hspace=0.0, wspace=0.0)\n",
    "\n",
    "plt.savefig('Main_Sequence_RNN_16x2+8x2.png', dpi=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18.0,6.0))\n",
    "\n",
    "axis = [10.5,15.2,7.1,12.8]\n",
    "\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_original,0.0,0.5,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=1,axis=axis,barposition=[0.96,0.15,0.03,0.80],modelname='Emerge')\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_original,0.5,1.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=2,axis=axis)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_original,1.0,2.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=3,axis=axis)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_original,2.0,4.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=4,axis=axis)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_original,4.0,8.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=5,axis=axis)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_pred_original,0.0,0.5,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=6,axis=axis,modelname='RNN',showredshift=False)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_pred_original,0.5,1.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=7,axis=axis,showredshift=False)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_pred_original,1.0,2.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=8,axis=axis,showredshift=False)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_pred_original,2.0,4.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=9,axis=axis,showredshift=False)\n",
    "plo.plot_shmr_panel_RNN(fig,X_original,y_pred_original,4.0,8.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=10,axis=axis,showredshift=False)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.89, bottom=0.14, top=0.99, hspace=0.0, wspace=0.0)\n",
    "\n",
    "plt.savefig('Shmr_RNN_RNN_16x2+8x2.png', dpi=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_hist =['F_2_hist.h5','F_3_hist.h5','F_4_hist.h5','F_5_hist.h5','F_6_hist.h5',\n",
    "              'F_7_hist.h5','F_8_hist.h5','F_9_hist.h5','F_10_hist.h5','F_11_hist.h5']\n",
    "        \n",
    "models =['F_2.h5','F_3.h5','F_4.h5','F_5.h5','F_6.h5',\n",
    "         'F_7.h5','F_8.h5','F_9.h5','F_10.h5','F_11.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_hist =['4_hist.h5','2x8_hist.h5','2x8+2x4_hist.h5',\n",
    "              '2x16+2x8_hist.h5','4x16+4x8_hist.h5','2x32_hist.h5',\n",
    "              '64_hist.h5','2x32+2x16_hist.h5','4x32_hist.h5',\n",
    "              '4x32+4x16_hist.h5']\n",
    "       \n",
    "models =['4.h5','2x8.h5','2x8+2x4.h5',\n",
    "         '2x16+2x8.h5','4x16+4x8.h5','2x32.h5',\n",
    "         '64.h5','2x32+2x16.h5','4x32.h5',\n",
    "         '4x32+4x16.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.plot_compare_models2(models_hist, models, ymin=0.004, ymax=0.06,fs=22,lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mstar_integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = tf.keras.models.load_model('Master/final_models/Master_model_RNN_RL_3rd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_labels, unscaled_predictions = cat.unscale_timeserieses(y,y_pred, galaxy_labels_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/del1.pkl', 'wb') as f:\n",
    "#     pickle.dump(unscaled_predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_Data/del1.pkl', 'rb') as f:\n",
    "    unscaled_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduced_trees_combined2 = cat.add_unscaled_predictions(Reduced_trees_combined, unscaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('pkl_Data/del2.pkl', 'wb') as f:\n",
    "#     pickle.dump(Reduced_trees_combined2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_Data/del2.pkl', 'rb') as f:\n",
    "    Reduced_trees_combined2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_trees, indices = cat.create_full_merger_tree(Reduced_trees_combined2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('Master/Preprocess/Full_trees_model_RNN_RL_3rd.pkl', 'wb') as f:\n",
    "#     pickle.dump(Full_trees, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/Full_trees_model_RNN_RL_3rd.pkl', 'rb') as f:\n",
    "    Full_trees = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_trees_reduced = []\n",
    "for tree in Full_trees:\n",
    "    tree2 = tree[['Scale','ID','Up_ID','Desc_ID','Main_ID',\n",
    "                  'Coprog_ID','Leaf_ID','Num_prog','Stellar_mass','SFR','mstar_pred','sfr_pred']]\n",
    "    tree2 = tree2.sort_values(by=['Scale'])\n",
    "    Full_trees_reduced.append(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('Master/Preprocess/Full_trees_reduced_model_RNN_RL_3rd.pkl', 'wb') as f:\n",
    "#     pickle.dump(Full_trees_reduced, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/Full_trees_reduced_model_RNN_RL_3rd.pkl', 'rb') as f:\n",
    "    Full_trees_reduced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mstar = cat.calculate_mstar(Full_trees_reduced, a_scales, exsitu = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('Master/Preprocess/exsitu_all_RL.pkl', 'wb') as f:\n",
    "#     pickle.dump(calculate_mstar, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/exsitu_all_RL.pkl', 'rb') as f:\n",
    "    calculate_mstar = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mstar_only_insitu  = cat.calculate_mstar(Full_trees_reduced, a_scales, exsitu = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pkl \n",
    "# with open('Master/Preprocess/insitu_all_RL.pkl', 'wb') as f:\n",
    "#     pickle.dump(calculate_mstar_only_insitu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/insitu_all_RL.pkl', 'rb') as f:\n",
    "    calculate_mstar_only_insitu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsitu =  [item for sublist in calculate_mstar for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insitu =  [item for sublist in calculate_mstar_only_insitu for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstar_label = []\n",
    "for i in Full_trees_reduced:\n",
    "    mstar_label.append(i['Stellar_mass'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstar_label2 = [item for sublist in mstar_label for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstar_pred = []\n",
    "for i in Full_trees_reduced:\n",
    "    mstar_pred.append(i['mstar_pred'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstar_pred2 = [item for sublist in mstar_pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_calculated_mstar(exsitu,insitu, mstar_pred2, mstar_label2,compare='sum', lw=3,fs=25,file='calculated_mstar_combined_mean2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Main_branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/Full_trees_model_RNN_RL_3rd.pkl', 'rb') as f:\n",
    "    Full_trees = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees, main_branches, indices_list, len_list = cat.get_example_main_branches(Full_trees, number_trees=10, len_trees=100)\n",
    "# low = 100\n",
    "# high = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_trees_reduced = []\n",
    "for tree in trees:\n",
    "    tree2 = tree[['Scale','ID','Up_ID','Desc_ID','Main_ID',\n",
    "                  'Coprog_ID','Leaf_ID','Num_prog','Stellar_mass','SFR','mstar_pred','sfr_pred','Main_galaxies']]\n",
    "    tree2 = tree2.sort_values(by=['Scale'])\n",
    "    Full_trees_reduced.append(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mstar = cat.calculate_mstar(Full_trees_reduced, a_scales, indices_list, exsitu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for counter, tree in enumerate(Full_trees_reduced):\n",
    "    tree['mstar_integrated'] = calculate_mstar[counter]\n",
    "    tree2 = tree[tree['Main_galaxies'] == 1.0]\n",
    "    tree3 = tree2[['Scale','mstar_pred','Stellar_mass','mstar_integrated']]\n",
    "    dfs.append(tree3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pkl \n",
    "with open('Master/Preprocess/example10_RNN_RL_low.pkl', 'wb') as f:\n",
    "    pickle.dump(dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Master/Preprocess/example10_RNN_low.pkl', 'rb') as f:\n",
    "    example10_RNN = pickle.load(f)\n",
    "\n",
    "with open('Master/Preprocess/example10_RNN_RL_low.pkl', 'rb') as f:\n",
    "    example10_RNN_RL = pickle.load(f)\n",
    "\n",
    "with open('Master/Preprocess/example10_NN_low.pkl', 'rb') as f:\n",
    "    example10_NN = pickle.load(f)\n",
    "    \n",
    "with open('Master/Preprocess/example10_NN_low_HaloNet.pkl', 'rb') as f:\n",
    "    example10_NN_RL = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.plot_main_branches(example10_RNN, example10_RNN_RL, example10_NN, example10_NN_RL, file='low.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "example10_RNN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example10_RNN_RL[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Baryon Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cat.calculate_baryon_df(tf.keras.models.load_model('Master/final_models/Master_model_RNN_RL_3rd.h5'), X, y, halo_features_used, galaxy_labels_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only stellar mass as color coding\n",
    "plo.plot_baryon_efficiency(df, file='test_bayron_compare.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list = {\n",
    "  'Stellar_mass': None,\n",
    "  'SFR': 'log',\n",
    "#  'Redshift': None,\n",
    "#  'Scale': None,\n",
    "  'Halo_mass': None,\n",
    "  'Halo_peak_mass': None,\n",
    "  'Halo_Growth_rate': 'log',\n",
    "  'Halo_growth_peak': 'log',\n",
    "#  'Scale_peak_mass': None,\n",
    "  'Concentration': 'log',\n",
    "#  'Main': None,\n",
    "#  'Central': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plo.plot_baryon_efficiency_2(df, compare_list=compare_list, file='test_bayron_compare1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load observed statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe    = obs.load_statistics_file('statistics.h5') #It contains ['CSFRD', 'Chi2', 'Clustering', 'FQ', 'Model_Parameters', 'SMF', 'SSFR']\n",
    "smf         = obs.average_smf_in_z_bins(universe,file_redshift,mstar_bins)\n",
    "fq          = obs.average_fq_in_z_bins(universe,file_redshift,mstar_bins)\n",
    "csfrd       = obs.average_csfrd_in_z_bins(universe,file_redshift)\n",
    "ssfr        = obs.average_ssfr_in_z_bins(universe,file_redshift,mstar_bins)\n",
    "wp, wp_mass = obs.get_clustering_data(universe,wp_start,wp_stop) #wp.shape=(n_attributs,n_sets,n_attr_entries) #wp_mass.shape=(n_sets, n_attr(min/max mass))\n",
    "\n",
    "#Set the minimum and maximum radius for the correlation functions according to the observed values\n",
    "rmin        = np.min(wp[0][wp[0]>0.0])\n",
    "rmax        = np.max(wp[0][wp[0]>0.0]) -20\n",
    "nrbin       = wp.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load positions and set bin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions          = cat.load_positions_RNN(whole_dataset)\n",
    "mstar_bin_edges    = obs.get_bin_edges(mstar_bins)\n",
    "redshift_bin_edges = obs.get_bin_edges(file_redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dictionary for values that will be passed to the statistics functions\n",
    "psodict = {\n",
    "    'model': None,\n",
    "    'modeltype_RNN': True,\n",
    "    'X_RNN_input': X,\n",
    "    'X': halos_scaled,\n",
    "    'halos': halos,\n",
    "    'galaxies': galaxies,\n",
    "    'positions': positions,\n",
    "    'pos_RNN_input': pos,\n",
    "    'galaxy_labels_used': galaxy_labels_used,\n",
    "    'halo_features_used': halo_features_used,\n",
    "    'mstar_bin_edges': mstar_bin_edges,\n",
    "    'redshift_bin_edges': redshift_bin_edges,\n",
    "    'obssigma0': obssigma0,\n",
    "    'obssigmaz': obssigmaz,\n",
    "    'zmax_sig': zmax_sig,\n",
    "    'ssfrthre': ssfrthre,\n",
    "    'H0': H0,\n",
    "    'Om0': Om0,\n",
    "    'Lbox': Lbox,\n",
    "    'dmstar': dmstar,\n",
    "    'ssfrmin': ssfrmin,\n",
    "    'wp_redshift': wp_redshift,\n",
    "    'wp_mass': wp_mass,\n",
    "    'rmin': rmin,\n",
    "    'rmax': rmax,\n",
    "    'nrbin': nrbin,\n",
    "    'smf': smf,\n",
    "    'fq': fq,\n",
    "    'ssfr': ssfr,\n",
    "    'csfrd': csfrd,\n",
    "    'wp': wp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute statistics for the galaxy catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_em,smf_sig_em,fq_em,fq_sig_em,ssfr_em,ssfr_sig_em,csfrd_em,csfrd_sig_em,wp_em \\\n",
    "    = rl.compute_statistics(psodict=psodict)\n",
    "rl.get_chi2(psodict=psodict,printchi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_list_em = np.round([1046.2301317161969, 496.9890074660465,267.76771598281334,40.44518624345288, 202.77466745905875, 38.253554564825336],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fit the parameters with Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Start with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load a RNN model to start with\n",
    "model            = tf.keras.models.load_model('Master/final_models/Master_model_RNN_RL_2nd.h5')\n",
    "best_parameters  = rl.get_weights(model, psodict)\n",
    "psodict['model'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Particle Swarms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Particles Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create the swarm and define some fitting parameters\n",
    "swarm = opt.PSOSwarm(n_particles=50, start_position=best_parameters,init_pos=0.05,seed=44,w=0.9,w_min=0.5,c1=0.9,c2=0.9)\n",
    "\n",
    "#Train the swarm\n",
    "swarm.train(rl.pso_loss,psodict=psodict, n_iterations=40, hist_file='pso.history.03.h5', gbest_file='pso.gbest.03.h5', gstop=0.5, n_loss=30)\n",
    "\n",
    "#Get the best parameters from the swarm\n",
    "best_pso_parameters = swarm.gbest_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Multi Particles Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### n_particles must be >= 2 ###\n",
    "Multi_swarm = opt.MSO(n_swarms=6, n_particles=8, start_position=best_parameters, \n",
    "                  mso_type='neutral', q=1.05, init_pos=0.003, pDeath = 0.005, pSwap = 0.005, w_max=0.9, \n",
    "                  w_min=0.5, c1_max=1.0, c1_min=0.8, c2_max=1.0, c2_min=0.8,\n",
    "                  c3_max=0.45, c3_min=0.9, q_desc=False, RC=1, RP=10) #mso_types: charged, atomic, neutral\n",
    "                                                         # q_desc implements a exp decay of the charge to q=1                                                      \n",
    "\n",
    "Multi_swarm.train_MSO(rl.pso_loss, psodict, n_iterations=50, phase=True, gbest_file='Master_mso_2ndrun.h5') \n",
    "# if phase == True, c3 will be set to zero for half of the iterations\n",
    "best_mso_parameters = Multi_swarm.gbest_position\n",
    "\n",
    "# 1st run:Multi_swarm = opt.MSO(n_swarms=6, n_particles=8, start_position=best_parameters, \n",
    "#                   mso_type='atomic', q=1.05, init_pos=0.005, pDeath = 0.005, pSwap = 0.01, w_max=0.9, \n",
    "#                   w_min=0.4, c1_max=1.0, c1_min=0.8, c2_max=1.0, c2_min=0.8,\n",
    "#                   c3_max=0.45, c3_min=0.9, q_desc=False, RC=1, RP=10) #mso_types: charged, atomic, neutral\n",
    "#                                                          # q_desc implements a exp decay of the charge to q=1  \n",
    "#Multi_swarm.train_MSO(rl.pso_loss, psodict, n_iterations=50, gbest_file='RL_RNN/X_9_mso.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gbestfile = hdf5.File('Master/final_models/Master_model_RNN_RL_3rd_hist.h5','r')\n",
    "filekeys  = [key for key in gbestfile.keys()]\n",
    "best_mso_parameters = np.array(gbestfile['Best_position']).flatten()\n",
    "best_mso_loss       = np.array(gbestfile['Best_loss'])\n",
    "swarm_best_positions = np.array(gbestfile['Swarm_best_positions']) \n",
    "swarm_best_losses = np.array(gbestfile['Swarm_best_losses']) \n",
    "swarm_history = np.array(gbestfile['Swarm_history'])\n",
    "\n",
    "gbestfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plo.plot_mso_history2(file1='Master/final_models/Master_model_RNN_RL_hist.h5', \n",
    "                 file2='Master/final_models/Master_model_RNN_RL_2nd_hist.h5',\n",
    "                 file3='Master/final_models/Master_model_RNN_RL_3rd_hist.h5',\n",
    "                 savefile='mso_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opt2 = opt.simulated_annealing(best_mso_parameters)\n",
    "\n",
    "opt2.train(rl.pso_loss, psodict, scale_max=0.00005, scale_min=0.00005, maxsteps=1000, T0 = 0.1, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Brute Force Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "asd = opt.optimizer(best_mso_parameters)\n",
    "asd.train(rl.pso_loss, psodict, pos=2, delta=1, scale=0.1, gbest_file='optimizer.h5')\n",
    "# pos = starting pos in parameter space, delta =number of changed parameters at one iteration, \n",
    "# scale =  range parameters are changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Either save the model...\n",
    "rl.set_weights(model,best_mso_parameters,psodict)\n",
    "\n",
    "model.save('Master/final_models/Master_model_RNN_RL_3rd.h5')\n",
    "\n",
    "# #...or repeat the fitting (continue at previous cell afterwards)\n",
    "# best_parameters = Multi_swarm.gbest_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load a PSO history and best parameters from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #Load PSO history from file\n",
    "# historyfile = hdf5.File('pso/RNN_Merger_2x140+60(b=50)_pso.history.03.h5','r')\n",
    "# filekeys    = [key for key in historyfile.keys()]\n",
    "# history_pos = np.array(historyfile['Positions'])\n",
    "# history_vel = np.array(historyfile['Velocities'])\n",
    "# historyfile.close()\n",
    "\n",
    "#Load best PSO position and value from file\n",
    "gbestfile = hdf5.File('Master/final_models/Master_model_RNN_RL_3rd_hist.h5','r')\n",
    "filekeys  = [key for key in gbestfile.keys()]\n",
    "best_pso_parameters = np.array(gbestfile['Best_position']).flatten()\n",
    "best_pso_loss       = np.array(gbestfile['Best_loss'])\n",
    "gbestfile.close()\n",
    "\n",
    "#Select a model and write the best parameters to it\n",
    "model           = tf.keras.models.load_model('Master/final_models/Master_model_RNN.h5')\n",
    "rl.set_weights(model,best_pso_parameters, psodict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the global statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = tf.keras.models.load_model('Master/final_models/Master_model_RNN_RL_3rd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the predictions\n",
    "y_pred  = model.predict(X, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero padded values and unscale the features and targets\n",
    "gal_pred, halos, positions, gal  = cat.data_without_zeropadding_RL(X, y, y_pred, galaxy_labels_used, halo_features_used, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed the predictions to the dictionary\n",
    "psodict['positions']= positions\n",
    "psodict['galaxies'] = gal_pred\n",
    "psodict['halos'] = halos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the statistics for this model\n",
    "smf_mod,smf_sig_mod,fq_mod,fq_sig_mod,ssfr_mod,ssfr_sig_mod,csfrd_mod,csfrd_sig_mod,wp_mod \\\n",
    "    = rl.compute_statistics(psodict=psodict)\n",
    "\n",
    "rl.get_chi2(psodict=psodict,printchi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_list_mod = np.round([655.7301888784817, 265.6890717148436, 188.89909819609446, 32.03477032426668, 131.38075114897023, 37.72649749430668],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf_obs_plot = smf.copy()\n",
    "smf_obs_plot[0,0,-2] = smf_obs_plot[0,0,-2] - 0.2\n",
    "plo.plot_smf(universe,file_redshift,mstar_bins,smf_obs_plot,smf_mod, smf_em, smf_huge=None,nx=5,ny=2,plotfile='GalaxyNet_RL_SMF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshifts_fq = np.array([file_redshift[2],file_redshift[3],file_redshift[4],file_redshift[5],file_redshift[6]])\n",
    "fq_plot      = fq[:,[2,3,4,5,6],:]\n",
    "fq_mod_plot  = fq_mod[[2,3,4,5,6],:]\n",
    "fq_em_plot   = fq_em[[2,3,4,5,6],:]\n",
    "\n",
    "plo.plot_fq(universe,redshifts_fq,mstar_bins,fq_plot,fq_mod_plot,fq_em_plot,nx=5,ny=1,plotfile='GalaxyNet_RL_FQ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstar_ssfr = np.array([mstar_bins[4],mstar_bins[6],mstar_bins[9],mstar_bins[11]])\n",
    "ssfr_plot     = ssfr[:,:,[4,6,9,11]]\n",
    "ssfr_mod_plot = ssfr_mod[:,[4,6,9,11]]\n",
    "ssfr_em_plot  = ssfr_em[:,[4,6,9,11]]\n",
    "\n",
    "plo.plot_ssfr(universe,file_redshift,mstar_ssfr,ssfr_plot,ssfr_mod_plot,ssfr_em_plot,nx=4,ny=1,plotfile='GalaxyNet_RL_SSFR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.plot_csfrd(universe,file_redshift,csfrd,csfrd_mod, csfrd_em,plotfile='GalaxyNet_RL_CSFRD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.plot_wp(universe,nx=4,ny=1,rad=wp[0,1],model=wp_mod,compare=wp_em,plotfile='GalaxyNet_RL_WP.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo.compare_chi2(chi2_list_em, chi2_list_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Main Sequence and the SHMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18.0,6.0))\n",
    "\n",
    "axis = [7.0,12.5,-5.9,2.9]\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal,0.0,0.5,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=1,axis=axis,barposition=[0.96,0.15,0.03,0.80],modelname='Emerge', Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal,0.5,1.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=2,axis=axis, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal,1.0,2.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=3,axis=axis, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal,2.0,4.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=4,axis=axis, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal,4.0,8.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=5,axis=axis, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal_pred,0.0,0.5,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=6,axis=axis,modelname='RNN + RL',showredshift=False, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal_pred,0.5,1.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=7,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal_pred,1.0,2.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=8,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal_pred,2.0,4.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=9,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_main_sequence_panel_RNN_RL(fig,halos,gal_pred,4.0,8.0,0,halo_features_used,galaxy_labels_used,H0=H0,Om0=Om0,plot_obs=True,nxpanel=5,nypanel=2,ipanel=10,axis=axis,showredshift=False, Unscale=False)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.89, bottom=0.14, top=0.99, hspace=0.0, wspace=0.0)\n",
    "\n",
    "plt.savefig('Main_Sequence_RNN_16x2+8x2_RL.png', dpi=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18.0,6.0))\n",
    "\n",
    "axis = [10.5,15.2,7.1,12.8]\n",
    "\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal,0.0,0.5,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=1,axis=axis,barposition=[0.96,0.15,0.03,0.80],modelname='Emerge', Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal,0.5,1.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=2,axis=axis, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal,1.0,2.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=3,axis=axis, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal,2.0,4.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=4,axis=axis, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal,4.0,8.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=5,axis=axis, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal_pred,0.0,0.5,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=6,axis=axis,modelname='RNN + RL',showredshift=False, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal_pred,0.5,1.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=7,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal_pred,1.0,2.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=8,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal_pred,2.0,4.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=9,axis=axis,showredshift=False, Unscale=False)\n",
    "plo.plot_shmr_panel_RNN_RL(fig,halos,gal_pred,4.0,8.0,0,halo_features_used,galaxy_labels_used,nxpanel=5,nypanel=2,ipanel=10,axis=axis,showredshift=False, Unscale=False)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.89, bottom=0.14, top=0.99, hspace=0.0, wspace=0.0)\n",
    "\n",
    "plt.savefig('RNN_merger_128+64shmr_RL.png', dpi=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "340.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
